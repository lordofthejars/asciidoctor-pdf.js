== Persistence

Quarkus works with JPA(Hibernate) as persistence solution.
But also provides an https://en.wikipedia.org/wiki/Active_record_pattern[Active Record pattern, window="_blank"] implementation under Panache project.

To use database access you need to add Quarkus JDBC drivers instead of the original ones.
At this time `Apache Derby`, `H2`, `MariaDB`, `MySQL`, `MSSQL` and `PostgreSQL` drivers are supported.

[source, bash]
----
./mvnw quarkus:add-extension 
  -Dextensions="io.quarkus:quarkus-hibernate-orm-panache, 
                io.quarkus:quarkus-jdbc-mariadb"
----

[source, java]
----
@Entity
public class Developer extends PanacheEntity {

    // id field is implicit

    public String name;
}
----

And configuration in `src/main/resources/application.properties`:

[source, properties]
----
quarkus.datasource.jdbc.url=jdbc:mariadb://localhost:3306/mydb
quarkus.datasource.db-kind=mariadb
quarkus.datasource.username=developer
quarkus.datasource.password=developer
quarkus.hibernate-orm.database.generation=update
----

// tag::update_14_42[]
List of datasource parameters.

`quarkus.datasource` as prefix is skipped in the next table.

`db-kind`::
Built-in datasource kinds so the JDBC driver is resolved automatically. Possible values: `derby`, `h2`, `mariadb`, `mssql`, `mysql`, `postgresql`, `db2`.

`username`::
Username to access.

`password`::
Password to access.

`driver`::
JDBC Driver class. It is not necessary to set if `db-kind` used.

`credentials-provider`::
Sets a custom credential provider name.

`credentials-provider-name`::
It is the `@Named` value of the credentials provider bean. Not necessary if only one implementation.

`jdbc.url`::
The datasource URL.

`jdbc.min-size`::
The datasource pool minimum size. (default: `0`)

`jdbc.max-size`::
The datasource pool maximum size. (default: `20`)

`jdbc.initial-size`::
The initial size of the pool.

`jdbc.background-validation-interval`::
The interval at which we validate idle connections in the background. (default: `2M`)

`jdbc.acquisition-timeout`::
The timeout before cancelling the acquisition of a new connection. (default: `5`)

`jdbc.leak-detection-interval`::
The interval at which we check for connection leaks.

`jdbc.idle-removal-interval`::
The interval at which we try to remove idle connections. (default: `5M`)

`jdbc.max-lifetime`::
The max lifetime of a connection.

`jdbc.transaction-isolation-level`::
The transaction isolation level. Possible values: `UNDEFINED`, `NONE`, `READ_UNCOMMITTED`, `READ_COMMITTED`, `REPEATABLE_READ`, `SERIALIZABLE`.

`jdbc.detect-statement-leaks`::
 Warn when a connection is returned to the pool without the application having closed all open statements. (default: `true`)

`jdbc.new-connection-sql`::
Query executed when first using a connection.

`jdbc.validation-query-sql`::
Query executed to validate a connection.

`jdbc.pooling-enabled`::
Disable pooling to prevent reuse of Connections. (default: `true`)

`jdbc.enable-metrics`::
Enable datasource metrics collection when using `quarkus-smallrye-metrics` extension.

`jdbc.additional-jdbc-properties.<extraProperty>`::
Unspecified properties to be passed to the JDBC driver when creating new connections.
// end::update_14_42[]

// tag::update_5_3[]
Hibernate configuration properties.
Prefix `quarkus.hibernate-orm` is skipped.

`dialect`::
Class name of the Hibernate ORM dialect.

`dialect.storage-engine`::
The storage engine when the dialect supports multiple storage engines.

`sql-load-script`::
Name of the file containing the SQL statements to execute when starts.
`no-file` force Hibernate to skip SQL import. (default: `import.sql`)

`batch-fetch-size`::
The size of the batches. (default: -1 disabled)

`maxFetchDepth`::
The maximum depth of outer join fetch tree for single-ended associations.

`multitenant`::
Defines the method for multi-tenancy. Possible values: `DATABASE`, `NONE`, `SCHEMA`. (default: `NONE`)

`multitenant-schema-datasource`::
Defines the name of the data source to use in case of SCHEMA approach.

`query.query-plan-cache-max-size`::
The maximum size of the query plan cache.

`query.default-null-ordering`::
Default precedence of null values in `ORDER BY`. Possible values: `none`, `first`, `last`. (default: `none`)

`database.generation`::
Database schema is generation. Possible values: `none`, `create`, `drop-and-create`, `drop`, `update`. (default: `none`)

`database.generation.halt-on-error`::
Stop on the first error when applying the schema. (default: `false`)

`database.generation.create-schemas`::
Hibernate ORM should create the schemas automatically (for databases supporting them).

`database.default-catalog`::
Default catalog.

`database.default-schema`::
Default Schema.

`database.charset`::
Charset.

`jdbc.timezone`::
Time Zone JDBC driver.

`jdbc.statement-fetch-size`::
Number of rows fetched at a time.

`jdbc.statement-batch-size`::
Number of updates sent at a time.

`log.sql`::
Show SQL logs (default: `false`)

`log.jdbc-warnings`::

`statistics`::
Enable statiscs collection. (default: `false`)

`physical-naming-strategy`::
Class name of the Hibernate PhysicalNamingStrategy implementation.

`globally-quoted-identifiers`::
Should quote all identifiers. (default: `false`)
// end::update_5_3[]

// tag::update_14_20[]
`metrics-enabled`::
Metrics published with smallrye-metrics extension (default: `false`)

second-level-caching-enabled::
Enable/Disable 2nd level cache. (default: `true`)
// end::update_14_20[]

Database operations:

[source, java]
----
// Insert
Developer developer = new Developer();
developer.name = "Alex";
developer.persist();

// Find All
Developer.findAll().list();

// Hibernate Filters 
Person.findAll().filter("Person.hasName", Parameters.with("name", "Alex"));

// Find By Query
Developer.find("name", "Alex").firstResult();

// Delete
Developer developer = new Developer();
developer.id = 1;
developer.delete();

Person.deleteById(id);
// Delete By Query
long numberOfDeleted = Developer.delete("name", "Alex");
----

Remember to annotate methods with `@Transactional` annotation to make changes persisted in the database.

If queries start with the keyword `from` then they are treated as _HQL_ query, if not then next short form is supported:

* `order by` which expands to `from EntityName order by ...`
* `<columnName>` which expands to `from EntityName where <columnName>=?`
* `<query>` which is expanded to `from EntityName where <query>`

*Static Methods*

`findById`: `Object`::
Returns object or null if not found. Overloaded version with `LockModeType` is provided.

// tag::update_12_1[]
`findByIdOptional`: `Optional<Object>`::
Returns object or `java.util.Optional`.
// end::update_12_1[]

`find`: `String`, [`Object...`, `Map<String, Object>`, `Parameters`]::
Lists of entities meeting given query with parameters set. Returning a `PanacheQuery`.

`find`: `String`, `Sort`, [`Object...`, `Map<String, Object>`, `Parameters`]::
Lists of entities meeting given query with parameters set sorted by `Sort` attribute/s. Returning a `PanacheQuery`.

`findAll`:: 
Finds all entities. Returning a `PanacheQuery`.

`findAll`: `Sort`::
Finds all entities sorted by `Sort` attribute/s. Returning a `PanacheQuery`.

`list`: `String`, [`Object...`, `Map<String, Object>`, `Parameters`]::
Lists of entities meeting given query with parameters set. Returning a `List`.

`list`: `String`, `Sort`, [`Object...`, `Map<String, Object>`, `Parameters`]::
Lists of entities meeting given query with parameters set sorted by `Sort` attribute/s. Returning a `List`.

`listAll`:: 
Finds all entities. Returning a `List`.

`listAll`: `Sort`::
Finds all entities sorted by `Sort` attribute/s. Returning a `List`.

`stream`: `String`, [`Object...`, `Map<String, Object>`, `Parameters`]::
`java.util.stream.Stream` of entities meeting given query with parameters set.

`stream`: `String`, `Sort`, [`Object...`, `Map<String, Object>`, `Parameters`]::
`java.util.stream.Stream` of entities meeting given query with parameters set sorted by `Sort` attribute/s.

`streamAll`:: 
`java.util.stream.Stream` of all entities.

`streamAll`: `Sort`::
`java.util.stream.Stream` of all entities sorted by `Sort` attribute/s.

`count`:: 
Number of entities.

`count`: `String`, [`Object...`, `Map<String, Object>`, `Parameters`]::
Number of entities meeting given query with parameters set.

// tag::update_16_10[]
.filter: [String, Parameters]::
Enables a Hibernate filter during fetching of results for this query.
// end::update_16_10[]

`deleteAll`:: 
Number of deleted entities.

`delete`: `String`, [`Object...`, `Map<String, Object>`, `Parameters`]::
Number of deleted entities meeting given query with parameters set.

// tag::update_15_6[]
`deleteById`: `boolean`, [`Object`]::
Delete by id. Returns if deleted or not.
// end::update_15_6[]

`persist`: [`Iterable`, `Steram`, `Object...`]::
Persist object.

// tag::update_14_30[]
In case of using streams, remember to close them or use a `try/catch` block: `try (Stream<Person> persons = Person.streamAll())`.
// end::update_14_30[]

// tag::update_12_6[]
TIP: `find` methods defines a `withLock(LockModeType)` to define the lock type and `withHint(QueryHints.HINT_CACHEABLE, "true")` to define hints.
// end::update_12_6[]

*Named Queries*

// tag::update_15_18[]
[source, java]
----
@Entity
@NamedQuery(name = "Person.getByName", query = "from Person where name = :name")
public class Person extends PanacheEntity {
    
    public static Person findByName(String name){
        return find("#Person.getByName", name).firstResult();
    }
}
----
// end::update_15_18[]

*Pagination*

// tag::update_9_3[]
[source, java]
----
PanacheQuery<Person> livingPersons = Person
            .find("status", Status.Alive);
livingPersons.page(Page.ofSize(25));

// get the first page
List<Person> firstPage = livingPersons.list();
// get the second page
List<Person> secondPage = livingPersons.nextPage().list();
----
// end::update_9_3[]

*Range*

// tag::update_15_7[]
[source, java]
----
PanacheQuery<Person> livingPersons = Person
            .find("status", Status.Alive);
List<Person> secondRange = livingPersons.range(25, 49).list();
----

You cannot mix pagination and range.
// end::update_15_7[]

// tag::update_2_10[]
If entities are defined in external JAR, you need to enable in these projects the `Jandex` plugin in project.

[source, xml]
----
<plugin>
    <groupId>org.jboss.jandex</groupId>
    <artifactId>jandex-maven-plugin</artifactId>
    <version>1.0.3</version>
    <executions>
        <execution>
            <id>make-index</id>
            <goals>
                <goal>jandex</goal>
            </goals>
        </execution>
    </executions>
    <dependencies>
        <dependency>
            <groupId>org.jboss</groupId>
            <artifactId>jandex</artifactId>
            <version>2.1.1.Final</version>
        </dependency>
    </dependencies>
</plugin>
----
// end::update_2_10[]

// tag::update_16_5[]
Panache includes an annotation processor that enhance your entities.
If you disable annotation processors you might need to create a marker file on Panache archives at `META-INF/panache-archive.marker` manually.
// end::update_16_5[]

*Testing*

// tag::update_16_3[]
To mock using active record pattern:

[source,xml]
----
<dependency>
    <groupId>io.quarkus</groupId>
    <artifactId>quarkus-panache-mock</artifactId>
    <scope>test</scope>
</dependency>
----

[source, java]
----
@Test
public void testPanacheMocking() {
    PanacheMock.mock(Person.class);
    
    Mockito.when(Person.count()).thenReturn(23l);
    Assertions.assertEquals(23, Person.count());
    PanacheMock.verify(Person.class, Mockito.times(1)).count();
}
----
// end::update_16_3[]

*DevServices*

// tag::update_24_1[]
When testing or running in dev mode Quarkus can even provide you with a zero config database out of the box.
Depending on your database type you may need docker installed in order to use this feature.

The following open source databases:

* Postgresql (container)
* MySQL (container)
* MariaDB (container)
* H2 (in-process)
* Apache Derby (in-process)

To use DevServices don’t configure a database URL, username and password, Quarkus will provide the database and you can just start coding without worrying about config.

[source, properties]
----
quarkus.datasource.db-kind=mariadb
%prod.quarkus.datasource.jdbc.url=jdbc:mariadb://db:3306/mydb
%prod.quarkus.datasource.username=developer
%prod.quarkus.datasource.password=developer
----

Possible configuration values prefixed with `quarkus.datasource`:

`devservices`::
If devservices is enabled or not. (default: `true`)

`devservices.image-name`::
The container image name to use instead of the default one.

`devservices.properties`::
Generic properties that are added to the database connection URL.

// end::update_24_1[]

*DAO pattern*

// tag::update_2_12[]
Also supports _DAO_ pattern with `PanacheRepository<TYPE>`.

[source, java]
----
@ApplicationScoped
public class DeveloperRepository 
    implements PanacheRepository<Person> {
   public Person findByName(String name){
     return find("name", name).firstResult();
   }
}
----
// end::update_2_12[]

*EntityManager*
// tag::update_5_2[]
You can inject `EntityManager` in your classes:

[source, java]
----
@Inject
EntityManager em;

@Inject
org.hibernate.Session session;

@Inject
org.hibernate.SessionFactory sessionFactory;

em.persist(car);
----
// end::update_5_2[]

*Multiple datasources*

// tag::update_9_9[]
You can register more than one datasource.

[source, properties]
----
# default
quarkus.datasource.db-kind=h2
quarkus.datasource.jdbc.url=jdbc:h2:tcp://localhost/mem:default
....
# users datasource
quarkus.datasource.users.devservices=false
quarkus.datasource.users.db-kind=h2
quarkus.datasource.users.jdbc.url=jdbc:h2:tcp://localhost/mem:users
----

Notice that after `datasource` you set the datasource name, in previous case `users`.

You can inject then `AgroalDataSource` with `io.quarkus.agroal.DataSource`.

[source, java]
----
@DataSource("users")
AgroalDataSource dataSource1;
----
// end::update_9_9[]

*Flushing*

// tag::update_4_4[]
You can force flush operation by calling `.flush()` or `.persistAndFlush()` to make it in a single call.

IMPORTANT: This flush is less efficient and you still need to commit transaction.
// end::update_4_4[]

*Testing*

// tag::update_4_3[]

There is a Quarkus Test Resource that starts and stops H2 server before and after test suite.

Register dependency `io.quarkus:quarkus-test-h2:test`.

And annotate the test:

[source, java]
----
@QuarkusTestResource(H2DatabaseTestResource.class)
public class FlywayTestResources {
}
----
// end::update_4_3[]

*Transactions*

// tag::update_6_4[]
The easiest way to define your transaction boundaries is to use the `@Transactional` annotation.

Transactions are mandatory in case of none idempotent operations.

[source, java]
----
@Transactional
public void createDeveloper() {}
----

You can control the transaction scope:

* `@Transactional(REQUIRED)` (default): starts a transaction if none was started, stays with the existing one otherwise.

* `@Transactional(REQUIRES_NEW)`: starts a transaction if none was started; if an existing one was started, suspends it and starts a new one for the boundary of that method.

* `@Transactional(MANDATORY)`: fails if no transaction was started ; works within the existing transaction otherwise.

* `@Transactional(SUPPORTS)`: if a transaction was started, joins it ; otherwise works with no transaction.

* `@Transactional(NOT_SUPPORTED)`: if a transaction was started, suspends it and works with no transaction for the boundary of the method; otherwise works with no transaction.

* `@Transactional(NEVER)`: if a transaction was started, raises an exception; otherwise works with no transaction.

You can configure the default transaction timeout using `quarkus.transaction-manager.default-transaction-timeout` configuration property. By default it is set to 60 seconds.

You can set a timeout property, in seconds, that applies to transactions created within the annotated method by using `@TransactionConfiguration` annotation.

[source, java]
----
@Transactional
@TransactionConfiguration(timeout=40)
public void createDeveloper() {}
----

If you want more control over transactions you can inject `UserTransaction` and use a programmatic way.

[source, java]
----
@Inject UserTransaction transaction

transaction.begin();
transaction.commit();
transaction.rollback();
----
// end::update_6_4[]

// tag::update_16_9[]
You can implement your custom credentials provider (ie Azure KeyVault) to provide a username/password for the database connection. `Name` information is not necessary if there is only one custom credential provider.

[source, java]
----
@ApplicationScoped
@Unremovable
@Named("my-credentials-provider")
public class CustomCredentialsProvider implements CredentialsProvider {
    @Inject
    Config config;
	
    @Override
	public Properties getCredentials(String credentialsProviderName) {

        properties.put(CredentialsProvider.USER_PROPERTY_NAME, "hibernate_orm_test");
		properties.put(CredentialsProvider.PASSWORD_PROPERTY_NAME, "hibernate_orm_test");

    }
}
----

[source, properties]
----
quarkus.datasource.credentials-provider=
    custom
quarkus.datasource.credentials-provider-name=
    my-credentials-provider
----
// end::update_16_9[]

*Hibernate Multitenancy*

// tag::update_16_14[]
Multitenancy is supported using Schema or Database approach.
First you need to define how tenant is identified:

[source, java]
----
@RequestScoped
@Unremovable
public class CustomTenantResolver implements TenantResolver {

    @Inject
    RoutingContext context;
    
    @Override
    public String getDefaultTenantId() {
        return "base";
    }
    
    @Override
    public String resolveTenantId() {
    }

}
----

_Schema approach_

[source, properties]
----
quarkus.hibernate-orm.database.generation=none

quarkus.hibernate-orm.multitenant=SCHEMA
----

_Database approach_

[source, properties]
----
quarkus.hibernate-orm.database.generation=none

quarkus.hibernate-orm.multitenant=DATABASE

# default tenant
quarkus.datasource.base.db-kind=postgresql
quarkus.datasource.base.username=quarkus_test
...
# Tenant 'mycompany'
quarkus.datasource.mycompany.db-kind=postgresql
quarkus.datasource.mycompany.username=mycompany
quarkus.flyway.mycompany.locations=classpath:database/mycompany
...
----

If you need more dynamic approach implement: `@ApplicationScoped io.quarkus.hibernate.orm.runtime.tenant.TenantConnectionResolver`
// end::update_16_14[]

== Hibernate Envers

// tag::update_18_1[]
Quarkus supports Hibernate Envers.

[source, bash]
----
./mvnw quarkus:add-extension 
  -Dextensions="hibernate-envers"
----
// end::update_18_1[]

== REST Data Panache

// tag::update_16_21[]
REST Data with Panache extension can generate the basic CRUD endpoints for your entities and repositories.

[source, bash]
----
./mvnw quarkus:add-extension 
  -Dextensions="hibernate-orm-rest-data-panache"
----

You also need to add the JDBC driver extension and a JSON Marshaller (ie `resteasy-jackson`).

Then you can define interfaces for defining endpoints:

In case of Active Record pattern:

[source, java]
----
public interface DeveloperResource extends PanacheEntityResource<Developer, Long> {
}
----

In case of Repository:

[source, java]
----
public interface DeveloperResource extends PanacheRepositoryResource<DeveloperRepository, Developer, Long> {
}
----

Quarkus will generate automatically the implementation for you following the next rules:

* Default path is a hyphenated lowercase resource name without a suffix of `resource` or `controller`.
* `get(@PathParam("id"))`, `list`, `add(Developer)`, `update(@PathParam("id"), Developer)`, `delete(@PathParam("id"))`

You can customize these defaults by using `@ResourceProperties` and `@MethodProperties` annotations.

[source, java]
----
@ResourceProperties(hal = true, path = "my-developer", halCollectionName = "dev-collections")
public interface DeveloperResource extends PanacheEntityResource<Developer, Long> {
    @MethodProperties(path = "all")
    List<Developer> list();
    @MethodProperties(exposed = false)
    void delete(Long id);
}
----

If `hal` is `true`, you need to send the `Accept: application/hal+json` HTTP header to get the response.
// end::update_16_21[]

== Hibernate Reactive

// tag::update_17_4[]

[source, bash]
----
./mvnw quarkus:add-extension 
  -Dextensions="quarkus-hibernate-reactive, quarkus-resteasy-mutiny, "
----

Also you need to add the reactive driver (ie `quarkus-reactive-pg-client`).

You can use: `org.hibernate.reactive.mutiny.Mutiny` or `org.hibernate.reactive.stage.Stage`.

[source, java]
----
@Entity
@Table(name = "dev")
public class Developer {
}

@Inject
CompletionStage<Stage.Session> stageSession;

@Inject
Uni<Mutiny.Session> mutinySession;

public Uni<Long> reactivePersist() {
    return mutinySession
        .flatMap(s -> s.persist(new Developer(1, "Alex"))
        .flatMap(v -> session.flush())
        ....

}

public CompletionStage<Developer> reactiveFind() {
    return stageSession
        .thenCompose(session -> {
            session.find(Developer.class, 1);
        });
}
----
// end::update_17_4[]

== Infinispan
// tag::update_8_7[]
Quarkus integrates with https://infinispan.org/[Infinispan, window="_blank"]:

[source, bash]
----
./mvnw quarkus:add-extension 
  -Dextensions="infinispan-client"
----

Serialization uses a library called https://github.com/infinispan/protostream[Protostream, window="_blank"].

*Annotation based*

[source, java]
----
@ProtoFactory
public Author(String name, String surname) {
    this.name = name;
    this.surname = surname;
}

@ProtoField(number = 1)
public String getName() {
    return name;
}

@ProtoField(number = 2)
public String getSurname() {
    return surname;
}
----

Initializer to set configuration settings.

[source, java]
----
@AutoProtoSchemaBuilder(includeClasses = 
    { Book.class, Author.class }, 
    schemaPackageName = "book_sample")
interface BookContextInitializer 
        extends SerializationContextInitializer {
}
----

*User written based*

There are three ways to create your schema:

_Protofile_

Creates a `.proto` file in the `META-INF` directory.

[source, proto]
----
package book_sample;

message Author {
  required string name = 1;
  required string surname = 2;
}
----

In case of having a Collection field you need to use the `repeated` key (ie `repeated Author authors = 4`).

_In code_

Setting `proto` schema directly in a produced bean.

[source, java]
----
@Produces
FileDescriptorSource bookProtoDefinition() {
    return FileDescriptorSource
        .fromString("library.proto",   
                    "package book_sample;\n" +
                    "message Author {\n" +
                    "  required string name = 1;\n" +
                    "  required string surname = 2;\n" +
                    "}");
}
----

_Marshaller_

Using `org.infinispan.protostream.MessageMarshaller` interface.

[source, java]
----
public class AuthorMarshaller 
    implements MessageMarshaller<Author> {

   @Override
   public String getTypeName() {
      return "book_sample.Author";
   }

   @Override
   public Class<? extends Author> getJavaClass() {
      return Author.class;
   }

   @Override
   public void writeTo(ProtoStreamWriter writer, 
                    Author author) throws IOException {
      writer.writeString("name", author.getName());
      writer.writeString("surname", author.getSurname());
   }

   @Override
   public Author readFrom(ProtoStreamReader reader) 
        throws IOException {
      String name = reader.readString("name");
      String surname = reader.readString("surname");
      return new Author(name, surname);
   }
}
----

And producing the marshaller:

[source, java]
----
@Produces
MessageMarshaller authorMarshaller() {
    return new AuthorMarshaller();
}
----
// end::update_8_7[]

*Infinispan Embedded*

// tag::update_10_7[]
[source, bash]
----
./mvnw quarkus:add-extension 
  -Dextensions="infinispan-embeddedy"
----

<<<

Configuration in `infinispan.xml`:

[source, xml]
----
<local-cache name="quarkus-transaction">
   <transaction 
        transaction-manager-lookup=
        "org.infinispan.transaction.lookup.JBossStandaloneJTAManagerLookup"/>
</local-cache>
----

Set configuration file location in `application.properties`:

[source, properties]
----
quarkus.infinispan-embedded.xml-config=infinispan.xml
----

And you can inject the main entry point for the cache:

[source, java]
----
@Inject
org.infinispan.manager.EmbeddedCacheManager cacheManager;
----
// end::update_10_7[]

== Redis

// tag::update_18_4[]
Quarkus integrates with Redis.

[source, bash]
----
./mvnw quarkus:add-extension 
  -Dextensions="redis-client"
----

Configure Redis location:

[source, properties]
----
quarkus.redis.hosts=localhost:6379
----

You can use synchronous or reactive clients:

[source, java]
----
@Inject
RedisClient redisClient;

@Inject
ReactiveRedisClient reactiveRedisClient;
----

[source, java]
----
void increment(String key, Integer incrementBy) {
    redisClient.incrby(key, incrementBy.toString());
}

Uni<List<String>> keys() {
    return reactiveRedisClient
        .keys("*")
        .map(response -> {
            List<String> result = new ArrayList<>();
            for (Response r : response) {
                result.add(r.toString());
            }
            return result;
        });
}
----

// tag::update_20_2[]
*Multiple Redis Clients*

[source, properties]
----
quarkus.redis.hosts = localhost:6379
quarkus.redis.second.hosts = localhost:6379
----

[source, java]
----
@Inject
RedisClient defaultRedisClient;

@Inject
@RedisClientName("second")
RedisClient redisClient2;
----

// end::update_20_2[]

List of Redis parameters.

`quarkus.redis` as prefix is skipped in the next table.

`health.enabled`::
Health check is published in case the smallrye-health extension is present. (default: `true`) 

`password`::
The Redis password.

`hosts`::
The Redis hosts. (default: `localhost:6379`) 

`database`::
The Redis database. 

`timeout`::
The maximum delay to wait before a blocking command to redis server times out. (default: `10s`)

`ssl`::
Enables or disables the SSL on connect.

`clinet-type`::
The Redis client type. Possible values: `standalone`, `cluster`, `sentinel` (default: `standalone`)
// end::update_18_4[]

== Flyway

// tag::update_1_7[]
Quarkus integrates with https://flywaydb.org/[Flyway] to help you on database schema migrations.

[source, bash]
----
./mvnw quarkus:add-extension 
  -Dextensions="quarkus-flyway"
----

Then place migration files to the migrations folder (`classpath:db/migration`).

You can inject `org.flywaydb.core.Flyway` to programmatically execute the migration.

[source, java]
----
@Inject
Flyway flyway;

flyway.migrate();
----

Or can be automatically executed by setting `migrate-at-start` property to `true`.

[source, properties]
----
quarkus.flyway.migrate-at-start=true
----

List of Flyway parameters.

`quarkus.flyway` as prefix is skipped in the next table.

`clean-at-start`::
Execute Flyway clean command (default: `false`) 

`migrate-at-start`::
Flyway migration automatically (default: `false`)

`locations`::
CSV locations to scan recursively for migrations. Supported prefixes `classpath` and `filesystem` (default: `classpath:db/migration`).

`connect-retries`::
The maximum number of retries when attempting to connect (default: 0)

`schemas`::
CSV case-sensitive list of schemas managed (default: none)

`table`::
The name of Flyway’s schema history table (default: `flyway_schema_history`)

`out-of-order`::
Allows migrations to be run "out of order".

`ignore-missing-migrations`::
Ignore missing migrations when reading the history table.

`sql-migration-prefix`::
Prefix for versioned SQL migrations (default: `V`)

`repeatable-sql-migration-prefix::`
Prefix for repeatable SQL migrations (default: `R`)

`baseline-on-migrate`::
Only migrations above *baseline-version* will then be applied

`baseline-version`::
Version to tag an existing schema with when executing baseline (default: 1)

`baseline-description`::
Description to tag an existing schema with when executing baseline (default: `Flyway Baseline`)
// end::update_1_7[]

// tag::update_13_6[]
`validate-on-migrate`::
Validate the applied migrations against the available ones (default: `true`)
// end::update_13_6[]

// tag::update_18_6[]
`placeholder-prefix`::
Prefix of every placeholder (default: `${`)

`placeholder-suffix`::
Suffix of every placeholder (default: `}`)
// end::update_18_6[]

// tag::update_22_9[]
`callbacks`::
Comma-separated list of fully qualified class names of Callback implementations.
// end::update_22_9[]

`ignore-future-migrations`::
Ignore future migrations when reading the history table.

*Multiple Datasources*
// tag::update_12_7[]

To use multiple datasource in Flyway you just need to add the datasource name just after the `flyway` property:

[source, properties]
----
quarkus.datasource.users.jdbc.url=jdbc:h2:tcp://localhost/mem:users
quarkus.datasource.inventory.jdbc.url=jdbc:h2:tcp://localhost/mem:inventory
# ...

quarkus.flyway.users.schemas=USERS_TEST_SCHEMA
quarkus.flyway.inventory.schemas=INVENTORY_TEST_SCHEMA
# ...
----
// end::update_12_7[]

== Liquibase

// tag::update_14_37[]
Quarkus integrates with https://www.liquibase.org/[Liquibase] to help you on database schema migrations.

[source, bash]
----
./mvnw quarkus:add-extension 
  -Dextensions="quarkus-liquibase"
----

Then place changelog files to the (`src/main/resources/db`) folder.

You can inject `org.quarkus.liquibase.LiquibaseFactory` to programmatically execute the migration.

[source, java]
----
@Inject
LiquibaseFactory liquibaseFactory;

try (Liquibase liquibase = liquibaseFactory.createLiquibase()) {
    ...
}
----

Or can be automatically executed by setting `migrate-at-start` property to `true`.

[source, properties]
----
quarkus.liquibase.migrate-at-start=true
----

List of Liquibase parameters.

`quarkus.liquibase` as prefix is skipped in the next table.

change-log::
The change log file. `XML`, `YAML` , `JSON`, `SQL` formats supported. (default: `db/changeLog.xml`)

change-log-parameters."<parameter-name>"::
Liquibase changelog parameters.

migrate-at-start::
The migrate at start flag. (default: `false`)

validate-on-migrate::
The validate on update flag. (default: `false`)

clean-at-start::
The clean at start flag. (default: `false`)

contexts::
The list of contexts.

labels::
The list of labels.

database-change-log-table-name::
The database change log lock table name. (default: `DATABASECHANGELOG`)

database-change-log-lock-table-name::
The database change log lock table name. (default: `DATABASECHANGELOGLOCK`)

default-catalog-name::
The default catalog name.

default-schema-name::
The default schema name.

liquibase-catalog-name::
The liquibase tables catalog name.

liquibase-schema-name::
The liquibase tables schema name.

liquibase-tablespace-name::
The liquibase tables tablespace name.

*Multiple Datasources*
// tag::update_12_7[]

To use multiple datasource in Liquibase you just need to add the datasource name just after the `liquibase` property:

[source, properties]
----
quarkus.datasource.users.jdbc.url=jdbc:h2:tcp://localhost/mem:users
quarkus.datasource.inventory.jdbc.url=jdbc:h2:tcp://localhost/mem:inventory
# ...

quarkus.liquibase.users.schemas=USERS_TEST_SCHEMA
quarkus.liquibase.inventory.schemas=INVENTORY_TEST_SCHEMA
# ...
----
// end::update_14_37[]

== Hibernate Search
// tag::update_3_1[]

Quarkus integrates with https://www.elastic.co/products/elasticsearch[Elasticsearch, window="_blank"] to provide a full-featured full-text search using https://hibernate.org/search/[Hibernate Search, window="_blank"] API. 

[source, bash]
----
./mvnw quarkus:add-extension 
  -Dextensions="quarkus-hibernate-search-elasticsearch"
----

You need to annotate your model with Hibernate Search API to index it:

[source, java]
----
@Entity
@Indexed
public class Author extends PanacheEntity {

    @FullTextField(analyzer = "english")
    public String bio;

    @FullTextField(analyzer = "name")
    @KeywordField(name = "firstName_sort", 
        sortable = Sortable.YES, 
        normalizer = "sort")
    public String firstName;

    @OneToMany
    @IndexedEmbedded
    public List<Book> books;

}
----

IMPORTANT: It is not mandatory to use Panache.

You need to define the analyzers and normalizers defined in annotations.
You only need to implement `ElasticsearchAnalysisConfigurer` interface and configure it.

[source, java]
----
public class MyQuarkusAnalysisConfigurer 
            implements ElasticsearchAnalysisConfigurer {

    @Override
    public void configure(
        ElasticsearchAnalysisDefinitionContainerContext ctx) 
    {
            ctx.analyzer("english").custom()
                .withTokenizer("standard")
                .withTokenFilters("asciifolding", 
                    "lowercase", "porter_stem");

        ctx.normalizer("sort").custom() 
            .withTokenFilters("asciifolding", "lowercase");
    }
}
----

Use Hibernate Search in REST service:

[source, java]
----
public class LibraryResource {

    @Inject
    EntityManager em;

    @Transactional
    public List<Author> searchAuthors(
        @QueryParam("pattern") String pattern) { 
        return Search.getSearchSession(em)
            .search(Author.class)
            .predicate(f ->
                pattern == null || pattern.isEmpty() ?
                    f.matchAll() :
                    f.simpleQueryString()
                        .onFields("firstName", 
                            "lastName", "books.title")
                        .matching(pattern)
                )
            .sort(f -> f.byField("lastName_sort")
            .then().byField("firstName_sort"))
            .fetchHits();
    }
----

IMPORTANT: When not using Hibernate ORM, index data using `Search.getSearchSession(em).createIndexer()` `.startAndWait()` at startup time.

Configure the extension in `application.properties`:

[source, properties]
----
quarkus.hibernate-search.elasticsearch.version=7
quarkus.hibernate-search.elasticsearch.
    analysis-configurer=MyQuarkusAnalysisConfigurer
quarkus.hibernate-search.elasticsearch.
    automatic-indexing.synchronization-strategy=searchable
quarkus.hibernate-search.elasticsearch.
    index-defaults.lifecycle.strategy=drop-and-create
quarkus.hibernate-search.elasticsearch.
    index-defaults.lifecycle.required-status=yellow
----

List of Hibernate-Elasticsearch properties prefixed with `quarkus.hibernate-search.elasticsearch`:

`backends`::
Map of configuration of additional backends.

`version`::
Version of Elasticsearch

`analysis-configurer`::
Class or name of the neab used to configure.

`hosts`::
List of Elasticsearch servers hosts.

`username`::
Username for auth.

`password`::
Password for auth.

`connection-timeout`::
Duration of connection timeout.

`max-connections`::
Max number of connections to servers.

`max-connections-per-route`::
Max number of connections to server.

`indexes`::
Per-index specific configuration.

`discovery.enabled`::
Enables automatic discovery.

`discovery.refresh-interval`::
Refresh interval of node list.

`discovery.default-scheme`::
Scheme to be used for the new nodes.

`automatic-indexing.synchronization-strategy`::
Status for which you wait before considering the operation completed (`queued`,`committed` or `searchable`).

`automatic-indexing.enable-dirty-check`::
When enabled, re-indexing of is skipped if the changes are on properties that are not used when indexing. 

`index-defaults.lifecycle.strategy`::
Index lifecycle (`none`, `validate`, `update`, `create`, `drop-and-create`, `drop-abd-create-drop`)

`index-defaults.lifecycle.required-status`::
Minimal cluster status (`green`, `yellow`, `red`)

`index-defaults.lifecycle.required-status-wait-timeout`::
Waiting time before failing the bootstrap.

`index-defaults.refresh-after-write`::
Set if index should be refreshed after writes.

Possible annotations:

`@Indexed`::
Register entity as full text index

`@FullTextField`::
Full text search. Need to set an analyzer to split tokens.

`@KeywordField`::
The string is kept as one single token but can be normalized.

`IndexedEmbedded`::
Include the Book fields into the Author index.

`@ContainerExtraction`::
Sets how to extract a value from container, e.g from a `Map`.

`@DocumentId`::
Map an unusual entity identifier to a document identifier.

`@GenericField`::
Full text index for any supported type.

`@IdentifierBridgeRef`::
Reference to the identifier bridge to use for a `@DocumentId`.

`@IndexingDependency`::
How a dependency of the indexing process to a property should affect automatic reindexing.

`@ObjectPath`::


`@ScaledNumberField`::
For `java.math.BigDecimal` or `java.math.BigInteger` that you need higher precision.
// end::update_3_1[]

== Amazon DynamoDB
// tag::update_5_6[]
// tag::update_8_3[]

Quarkus integrates with https://aws.amazon.com/dynamodb/:

[source, bash]
----
./mvnw quarkus:add-extension 
  -Dextensions="quarkus-amazon-dynamodb"
----

[source, java]
----
@Inject
DynamoDbClient dynamoDB;
----

// tag::update_14_26[]
To use asycnhronous client with Mutiny:

[source, shell-session]
----
./mvnw quarkus:add-extension 
  -Dextensions="quarkus-amazon-dynamodb, resteasy-mutiny"
----

[source, java]
----
@Inject
DynamoDbAsyncClient dynamoDB;

Uni.createFrom().completionStage(() -> dynamoDB.scan(scanRequest()))....
----
// end::update_14_26[]

To use it as a local DynamoDB instance:

[source, properties]
----
quarkus.dynamodb.region=
    eu-central-1
quarkus.dynamodb.endpoint-override=
    http://localhost:8000
quarkus.dynamodb.credentials.type=STATIC
quarkus.dynamodb.credentials.static-provider
    .access-key-id=test-key
quarkus.dynamodb.credentials.static-provider
.secret-access-key=test-secret
----

If you want to work with an AWS account, you’d need to set it with:

[source, java]
----
quarkus.dynamodb.region=<YOUR_REGION>
quarkus.dynamodb.credentials.type=DEFAULT
----

`DEFAULT` credentials provider chain: 

* System properties `aws.accessKeyId`, `aws.secretKey`
* Env. Varables `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`
* Credentials profile `~/.aws/credentials`
* Credentials through the Amazon EC2 container service if the `AWS_CONTAINER_CREDENTIALS_RELATIVE_URI` set
* Credentials through Amazon EC2 metadata service.

Configuration parameters prefixed with `quarkus.dynamodb`:

|===	
|Parameter | Default | Description
a|`enable-endpoint-discovery`
a|`false`
a|Endpoint discovery for a service API that supports endpoint discovery.

a|`endpoint-override`
a|
a|Configure the endpoint with which the SDK should communicate.

a|`api-call-timeout`
a|
a|Time to complete an execution.

a|`interceptors`
a|
a|List of class interceptors.
|===

Configuration parameters prefixed with `quarkus.dynamodb.aws`:

|===	
|Parameter | Default | Description

a|`region`
a|
a|Region that hosts DynamoDB.

a|`credentials.type`
a|`DEFAULT`
a| Credentials that should be used `DEFAULT`, `STATIC`, `SYSTEM_PROPERTY`, `ENV_VARIABLE`, `PROFILE`, `CONTAINER`, `INSTANCE_PROFILE`, `PROCESS`, `ANONYMOUS`
|===

Credentials specific parameters prefixed with `quarkus.dynamodb.aws.credentials`:

|===	
|Parameter | Default | Description

3+|DEFAULT  

a|`default-provider.async-credential-update-enabled`
a|`false`
a|Should fetch credentials async.

a|`default-provider.reuse-last-provider-enabled`
a|`true`
a|Should reuse the last successful credentials.

3+|STATIC

a|`static-provider.access-key-id`
a|
a|AWS access key id.

a|`static-provider.secret-access-key`
a|
a|AWS secret access key.

3+|PROFILE

a|`profile-provider.profile-name`
a|`default`
a|The name of the profile to use.

3+|PROCESS

a|`process-provider.command`
a|
a|Command to execute to retrieve credentials.

a|`process-provider.process-output-limit`
a|1024
a|Max bytes to retrieve from process.

a|`process-provider.credential-refresh-threshold`
a|`PT15S`
a|The amount of time between credentials expire and credentials refreshed.

a|`process-provider.async-credential-update-enabled`
a|`false`
a|Should fetch credentials async.
|===

In case of synchronous client, the next parameters can be configured prefixed by `quarkus.dynamodb.sync-client`:

|===	
|Parameter | Default | Description

a|`connection-acquisition-timeout`
a|`10S`
a|Connection acquisation timeout.

a|`connection-max-idle-time`
a|`60S`
a|Max time to connection to be opened.

a|`connection-timeout`
a|
a|Connection timeout.

a|`connection-time-to-live`
a|`0`
a|Max time connection to be open.

a|`socket-timeout`
a|`30S`
a|Time to wait for data.

a|`max-connections`
a|`50`
a|Max connections.

a|`expect-continue-enabled`
a|`true`
a|Client send an HTTP `expect-continue` handsake.

a|`use-idle-connection-reaper`
a|`true`
a| Connections in pool should be closed asynchronously.

a|`proxy.endpoint`
a|
a|Endpoint of the proxy server.

a|`proxy.enabled`
a|`false`
a|Enables HTTP proxy.

a|`proxy.username`
a|
a|Proxy username.

a|`proxy.password`
a|
a|Proxy password.

a|`proxy.ntlm-domain`
a|
a|For NTLM, domain name.

a|`proxy.ntlm-workstation`
a|
a|For NTLM, workstation name.

a|`proxy.preemptive-basic-authentication-enabled`
a|
a|Authenticate pre-emptively.

a|`proxy.non-proxy-hosts`
a|
a|List of non proxy hosts.

a|`tls-managers-provider.type`
a|`system-property`
a|TLS manager: `none`, `system-property`, `file-store`

a|`tls-managers-provider.file-store.path`
a|
a|Path to key store.

a|`tls-managers-provider.file-store.type`
a|
a|Key store type.

a|`tls-managers-provider.file-store.password`
a|
a|Key store password.
|===

In case of asynchronous client, the next parameters can be configured prefixed by `quarkus.dynamodb.async-client`:

|===	
|Parameter | Default | Description

a|`connection-acquisition-timeout`
a|`10S`
a|Connection acquisation timeout.

a|`connection-max-idle-time`
a|`60S`
a|Max time to connection to be opened.

a|`connection-timeout`
a|
a|Connection timeout.

a|`connection-time-to-live`
a|`0`
a|Max time connection to be open.

a|`max-concurrency`
a|`50`
a|Max number of concurrent connections.

a|`use-idle-connection-reaper`
a|`true`
a|Connections in pool should be closed asynchronously.

a|`read-timeout`
a|`30S`
a|Read timeout.

a|`write-timeout`
a|`30S`
a|Write timeout.

a|`proxy.endpoint`
a|
a|Endpoint of the proxy server.

a|`proxy.enabled`
a|`false`
a|Enables HTTP proxy.

a|`proxy.non-proxy-hosts`
a|
a|List of non proxy hosts.

a|`tls-managers-provider.type`
a|`system-property`
a|TLS manager: `none`, `system-property`, `file-store`

a|`tls-managers-provider.file-store.path`
a|
a|Path to key store.

a|`tls-managers-provider.file-store.type`
a|
a|Key store type.

a|`tls-managers-provider.file-store.password`
a|
a|Key store password.

a|`ssl-provider`
a|
a|SSL Provider (`jdk`, `openssl`, `openssl-refcnt`).

a|`protocol`
a|`HTTP_1_1`
a|Sets the HTTP protocol.

a|`max-http2-streams`
a|
a|Max number of concurrent streams.

a|`event-loop.override`
a|`false`
a| Enable custom event loop conf.

a|`event-loop.number-of-threads`
a|
a|Number of threads to use in event loop.

a|`event-loop.thread-name-prefix`
a|`aws-java-sdk-NettyEventLoop`
a| Prefix of thread names.
|===
// end::update_5_6[]
// end::update_8_3[]

== Amazon S3

// tag::update_16_20[]
[source, bash]
----
./mvnw quarkus:add-extension 
  -Dextensions="quarkus-amazon-s3"
----

[source, java]
----
@Inject
S3Client s3Client;
----

You need to set a HTTP client either `URL Connection`:

[source, xml]
----
<dependency>
    <groupId>software.amazon.awssdk</groupId>
    <artifactId>url-connection-client</artifactId>
</dependency>
----

or Apache HTTP:

[source, xml]
----
<dependency>
    <groupId>software.amazon.awssdk</groupId>
    <artifactId>apache-client</artifactId>
</dependency>
----

[source, properties]
----
quarkus.s3.sync-client.type=apache
----

And configure it:

[source,properties]
----
quarkus.s3.endpoint-override=http://localhost:8008
quarkus.s3.interceptors=io.quarkus.it.amazon.s3.S3ModifyResponse
quarkus.s3.aws.region=us-east-1
quarkus.s3.aws.credentials.type=static
quarkus.s3.aws.credentials.static-provider.access-key-id=test-key
quarkus.s3.aws.credentials.static-provider.secret-access-key=test-secret
----

You can inject asynchronous client too:

[source, java]
----
@Inject
S3AsyncClient s3AsyncClient;
----

And you need to add the asynchronous Netty client:

[source, xml]
----
<dependency>
    <groupId>software.amazon.awssdk</groupId>
    <artifactId>netty-nio-client</artifactId>
</dependency>
----

Configuration properties are the same as <<Amazon DynamoDB>> but changing the prefix from `dynamodb` to `s3`.
// end::update_16_20[]

== Neo4j
// tag::update_5_7[]
Quarkus integrates with https://neo4j.com/[Neo4j, window="_blank"]:

[source, bash]
----
./mvnw quarkus:add-extension 
  -Dextensions="quarkus-neo4j"
----

[source, java]
----
@Inject
org.neo4j.driver.Driver driver;
----

Configuration properties:

`quarkus.neo4j` as prefix is skipped in the next table.

Prefix is `quarkus.neo4j`.

`uri`::
URI of Neo4j. (default: `localhost:7687`)

`authentication.username`::
Username. (default: `neo4j`)

`authentication.password`::
Password. (default: `neo4j`)

`authentication.disabled`::
Disable authentication. (default: `false`)

`pool.metrics-enabled`::
Enable metrics. (default: `false`)

`pool.log-leaked-sessions`::
Enable leaked sessions logging. (default:`false`)

`pool.max-connection-pool-size`::
Max amount of connections. (default: `100`)

`pool.max-connection-lifetime`::
Pooled connections older will be closed and removed from the pool. (default: `1H`)

`pool.connection-acquisition-timeout`::
Timout for connection adquisation. (default: 1M)

`pool.idle-time-before-connection-test`::
Pooled connections idled in the pool for longer than this timeout will be tested before they are used. (default: `-1`)

As Neo4j uses SSL communication by default, to create a native executable you need to compile with next options GraalVM options:

`-H:EnableURLProtocols=http,https --enable-all-security-services -H:+JNI`

And Quarkus Maven Plugin with next configuration:

[source, xml]
----
<artifactId>quarkus-maven-plugin</artifactId>
<executions>
    <execution>
        <id>native-image</id>
        <goals>
            <goal>native-image</goal>
        </goals>
        <configuration>
            <enableHttpUrlHandler>true
            </enableHttpUrlHandler>
            <enableHttpsUrlHandler>true
            </enableHttpsUrlHandler>
            <enableAllSecurityServices>true
            </enableAllSecurityServices>
            <enableJni>true</enableJni>                
        </configuration>
    </execution>
</executions>
----

Alternatively, and as a not recommended way in production, you can disable SSL and Quarkus will disable Bolt SSL as well. `quarkus.ssl.native=false`.

// end::update_5_7[]
// tag::update_14_19[]
If you are using Neo4j 4.0, you can use fully reactive. 
Add the next extension: `quarkus-resteasy-mutiny`.

[source, java]
----
@GET
public Publisher<String> get() {
    return Multi.createFrom().resource(driver::rxSession,
        session -> session.readTransaction(tx -> {
            RxResult result = tx.run("MATCH (f:Fruit) RETURN f.name as name");
            return Multi.createFrom().publisher(result.records())
                    .map(record -> record.get("name").asString());
        })
    ).withFinalizer(session -> {
        return Uni.createFrom().publisher(session.close());
    });
}
----
// end::update_14_19[]

== MongoDB Client
// tag::update_5_10[]
Quarkus integrates with https://www.mongodb.com/[MongoDB, window="_blank"]:

[source, bash]
----
./mvnw quarkus:add-extension 
  -Dextensions="quarkus-mongodb-client"
----

[source, java]
----
@Inject
com.mongodb.client.MongoClient client;

@Inject
io.quarkus.mongodb.reactive.ReactiveMongoClient client;
----

// tag::update_14_17[]
INFO: Reactive client uses exposes Mutiny API.
// end::update_14_17[]

[source, properties]
----
quarkus.mongodb.connection-string=mongodb://localhost:27018
quarkus.mongodb.write-concern.journal=false
----

*Multi MongoDB support*

// tag::update_14_5[]
You can configure multiple MongoDB clients using same approach as with `DataSource`.
The syntax is `quarkus.mongodb.<optional name>.<property>`:

[source, properties]
----
quarkus.mongodb.users.connection-string = mongodb://mongo2:27017/userdb
quarkus.mongodb.inventory.connection-string = mongodb://mongo3:27017/invdb
----

Inject the instance using `@io.quarkus.mongodb.runtime.MongoClientName` annotation:

[source, java]
----
@Inject
@MongoClientName("users")
MongoClient mongoClient1;
----
// end::update_14_5[]

<<<

`quarkus.mongodb` as prefix is skipped in the following table.

|===	
|Parameter | Type | Description

a|`connection-string`
a|`String`
a|MongoDB connection URI.

a|`hosts`
a|`List<String>`
a|Addresses passed as `host:port`.

a|`application-name`
a|`String`
|Application name.

a|`max-pool-size`
a|`Int`
|Maximum number of connections.

a|`min-pool-size`
a|`Int`
|Minimum number of connections.

a|`max-connection-idle-time`
a|`Duration`
|Idle time of a pooled connection.

a|`max-connection-life-time`
a|`Duration`
|Life time of pooled connection.

a|`wait-queue-timeout`
a|`Duration`
|Maximum wait time for new connection.

a|`maintenance-frequency`
a|`Duration`
|Time period between runs of maintenance job.

a|`maintenance-initial-delay`
a|`Duration`
|Time to wait before running the first maintenance job.

a|`wait-queue-multiple`
a|`Int`
a|Multiplied with `max-pool-size` gives max numer of threads waiting.

a|`connection-timeout`
a|`Duration`
|

a|`socket-timeout`
a|`Duration`
|

a|`tls-insecure`
a|`boolean [false]`
|Insecure TLS.

a|`tls`
a|`boolean [false]`
|Enable TLS

a|`replica-set-name`
a|`String`
|Implies hosts given are a seed list.

a|`server-selection-timeout`
a|`Duration`
|Time to wait for server selection.

a|`local-threshold`
a|`Duration`
|Minimum ping time to make a server eligible.

a|`heartbeat-frequency`
a|`Duration`
|Frequency to determine the state of servers.

a|`read-preference`
a|
`primary`,
`primaryPreferred`,
`secondary`,
`secondaryPreferred`,
`nearest`
|Read preferences.

a|`max-wait-queue-size`
a|`Int`
|Max number of concurrent operations allowed to wait.

a|`write-concern.safe`
a|`boolean [true]`
|Ensures are writes are ack.

a|`write-concern.journal`
a|`boolean [true]`
|Journal writing aspect.

a|`write-concern.w`
a|`String`
|Value to all write commands.

a|`write-concern.retry-writes`
a|`boolean [false]`
|Retry writes if network fails.

a|`write-concern.w-timeout`
a|`Duration`
|Timeout to all write commands.

a|`credentials.username`
a|`String`
|Username.

a|`credentials.password`
a|`String`
|Password.

a|`credentials.auth-mechanism`
a|`MONGO-CR`, `GSSAPI`, `PLAIN`, `MONGODB-X509`
|

a|`credentials.auth-source`
a|`String`
|Source of the authentication credentials.

a|`credentials.auth-mechanism-properties`
a|`Map<String, String>`
|Authentication mechanism properties.
|===
// end::update_5_10[]

== MongoDB Panache
// tag::update_9_2[]
You can also use the Panache framework to write persistence part when using MongoDB.

[source, bash]
----
./mvnw quarkus:add-extension 
  -Dextensions="mongodb-panache"
----

MongoDB configuration comes from <<MongoDB Client>> section.

[source, java]
----
@MongoEntity(collection="ThePerson")
public class Person extends PanacheMongoEntity {
    public String name;

    @BsonProperty("birth")
    public LocalDate birthDate;

    public Status status;
}
----

Possible annotations in fields: `@BsonId` (for custom ID), `@BsonProperty` and `@BsonIgnore`.

IMPORTANT: `@MongoEntity` is optional.

*Multi-tenancy with MongoDB Panache*

// tag::update_15_4[]
[source, java]
----
@MongoEntity(collection = "TheBook", clientName = "client2", database = "database2")
----
// end::update_15_4[]

Methods provided are similar of the ones shown in <<Persistence>> section.

[source, java]
----
person.persist();
person.update();
person.delete();

List<Person> allPersons = Person.listAll();
person = Person.findById(personId);
List<Person> livingPersons = Person.list("status", Status.Alive);
List<Person> persons = Person.list(Sort.by("name").and("birth"));

long updated = Person.update("name", "Mortal").where("status", Status.Alive);

long countAll = Person.count();

Person.deleteById(id);
Person.delete("status", Status.Alive);
----

All `list` methods have equivalent `stream` versions.

*Pagination*

You can also use pagination:

[source, java]
----
PanacheQuery<Person> livingPersons = 
    Person.find("status", Status.Alive);
livingPersons.page(Page.ofSize(25));

// get the first page
List<Person> firstPage = livingPersons.list();
// get the second page
List<Person> secondPage = livingPersons.nextPage().list();
----

*Range*

[source, java]
----
PanacheQuery<Person> livingPersons = Person
            .find("status", Status.Alive);
List<Person> secondRange = livingPersons.range(25, 49).list();
----

You cannot mix pagination and range.

*Queries*

Native MongoDB queries are supported (if they start with `{` or `org.bson.Document` instance) as well as Panache Queries.
Panache Queries equivalence in MongoDB:

* `firstname = ?1 and status = ?2` -> `{'firstname': ?1, 'status': ?2}`
* `amount > ?1 and firstname != ?2` -> `{'amount': {'$gt': ?1}, 'firstname': {'$ne': ?2}}`
* `lastname like ?1` -> `{'lastname': {'$regex': ?1}}`
* `lastname is not null` -> `{'lastname':{'$exists': true}}`

WARNING: PanacheQL refers to the Object parameter name but native queries refer to MongoDB field names.

*Projection*

// tag::update_13_10[]
Projection can be done for both PanacheQL and native queries.

[source, java]
----
import io.quarkus.mongodb.panache.ProjectionFor;

@ProjectionFor(Person.class) // <1>
public class PersonName {
    public String name;
}

PanacheQuery<PersonName> shortQuery = Person.find("status ", Status.Alive).project(PersonName.class);
----
<1> Entity class.
// end::update_13_10[]

*Testing*

To mock using active record pattern:

[source,xml]
----
<dependency>
    <groupId>io.quarkus</groupId>
    <artifactId>quarkus-panache-mock</artifactId>
    <scope>test</scope>
</dependency>
----

[source, java]
----
@Test
public void testPanacheMocking() {
    PanacheMock.mock(Person.class);
    
    Mockito.when(Person.count()).thenReturn(23l);
    Assertions.assertEquals(23, Person.count());
    PanacheMock.verify(Person.class, Mockito.times(1)).count();
}
----

*DAO pattern*

[source, java]
----
@ApplicationScoped
public class PersonRepository 
    implements PanacheMongoRepository<Person> {
}
----

*Jandex*

If entities are defined in external JAR, you need to enable in these projects the `Jandex` plugin in project.

[source, xml]
----
<plugin>
    <groupId>org.jboss.jandex</groupId>
    <artifactId>jandex-maven-plugin</artifactId>
    <version>1.0.3</version>
    <executions>
        <execution>
            <id>make-index</id>
            <goals>
                <goal>jandex</goal>
            </goals>
        </execution>
    </executions>
    <dependencies>
        <dependency>
            <groupId>org.jboss</groupId>
            <artifactId>jandex</artifactId>
            <version>2.1.1.Final</version>
        </dependency>
    </dependencies>
</plugin>
----
// end::update_9_2[]

Panache includes an annotation processor that enhance your entities.
If you disable annotation processors you might need to create a marker file on Panache archives at `META-INF/panache-archive.marker` manually.

*Reactive Panache*

// tag::update_14_12[]
MongoDB with Panache allows using reactive implementation too by using `ReactivePanacheMongoEntity` or `ReactivePanacheMongoEntityBase` or `ReactivePanacheMongoRepository` or `ReactivePanacheMongoRepositoryBase` depending on your style.

[source, java]
----
public class ReactivePerson extends ReactivePanacheMongoEntity {
    public String name;
}

CompletionStage<Void> cs1 = person.persist();
CompletionStage<List<ReactivePerson>> allPersons = ReactivePerson.listAll();
Publisher<ReactivePerson> allPersons = ReactivePerson.streamAll();

Uni<List<PersonName>> persons = ReactivePersonEntity.find("lastname", name).project(PersonName.class).list();
----
// end::update_14_12[]

== MongoDB REST Data Panache

// tag::update_21_2[]
MongoDB REST Data with Panache extension can generate the basic CRUD endpoints for your entities and repositories.

[source, bash]
----
./mvnw quarkus:add-extension 
  -Dextensions="mongodb-rest-data-panache"
----

You also need to add the JDBC driver extension and a JSON Marshaller (ie `resteasy-jackson`).

Then you can define interfaces for defining endpoints:

In case of Active Record pattern:

[source, java]
----
public interface DeveloperResource extends PanacheMongoEntityResource<Developer, Long> {
}
----

In case of Repository:

[source, java]
----
public interface DeveloperResource extends PanacheMongoRepositoryResource<DeveloperRepository, Developer, Long> {
}
----
// end::update_21_2[]

== Cassandra

// tag::update_17_12[]
Quarkus integrates with Cassandra and DataStax Object Mapper.

[source, xml]
----
<dependency>
  <groupId>com.datastax.oss.quarkus</groupId>
  <artifactId>cassandra-quarkus-client</artifactId>
</dependency>
----

Enities and DAOs are generated as you have been doing with DataStax Object Mapper.

You need to create a DaoProducer:

[source, java]
----
@Inject
public FruitDaoProducer(QuarkusCqlSession session) {
  FruitMapper mapper = new FruitMapperBuilder(session).build();
  fruitDao = mapper.fruitDao();
}

@Produces
@ApplicationScoped
FruitDao produceFruitDao() {
  return fruitDao;
}
----

Cassandra configuration:

[source, properties]
----
quarkus.cassandra.contact-points=127.0.0.1:9042
quarkus.cassandra.local-datacenter=datacenter1
quarkus.cassandra.keyspace=k1
quarkus.cassandra.auth.username=john
quarkus.cassandra.auth.password=s3cr3t
----

You can configure other Cassandra Java driver settings using `application.conf` or `application.json` files. They need to be located in the classpath of your application.
https://docs.datastax.com/en/developer/java-driver/latest/manual/core/configuration/reference/[Driver settings reference].

If MicroProfile Metrics extension is registered, the Cassandra extension can provide (if enabled) metrics about the session:

[source, properties]
----
quarkus.cassandra.metrics.enabled=true
quarkus.cassandra.metrics.session-enabled=connected-nodes,bytes-sent
quarkus.cassandra.metrics.node-enabled=pool.open-connections
----

*Reactive*

You can also use Mutiny to define a reactive DAO:

[source, java]
----
@Dao
public interface FruitDaoReactive {

  @Update
  Uni<Void> update(Fruit fruit);

  @Select
  MutinyMappedReactiveResultSet<Fruit> findById(String storeId);
}

@Mapper
public interface FruitMapper {

  @DaoFactory
  FruitDaoReactive fruitDaoReactive();
}
----
// end::update_17_12[]