== Persistence

Quarkus works with JPA(Hibernate) as persistence solution.
But also provides an https://en.wikipedia.org/wiki/Active_record_pattern[Active Record pattern, window="_blank"] implementation under Panache project.

To use database access you need to add Quarkus JDBC drivers instead of the original ones.
At this time `Apache Derby`, `H2`, `MariaDB`, `MySQL`, `MSSQL` and `PostgreSQL` drivers are supported.

[source, bash]
----
./mvnw quarkus:add-extension 
  -Dextensions="io.quarkus:quarkus-hibernate-orm-panache, 
                io.quarkus:quarkus-jdbc-mariadb"
----

[source, java]
----
@Entity
public class Developer extends PanacheEntity {

    // id field is implicit

    public String name;
}
----

And configuration in `src/main/resources/application.properties`:

[source, properties]
----
quarkus.datasource.url=jdbc:mariadb://localhost:3306/mydb
quarkus.datasource.driver=org.mariadb.jdbc.Driver
quarkus.datasource.username=developer
quarkus.datasource.password=developer
quarkus.hibernate-orm.database.generation=update
----

// tag::update_4_6[]
List of datasource parameters.

`quarkus.datasource` as prefix is skipped in the next table.

|===
|Parameter|Type

a|`driver`
a|`String`

a|`url`
a|`String`

a|`username`
a|`String`

a|`password`
a|`String`

a|`min-size`
a|`Integer`

a|`max-size`
a|`Integer`

a|`initial-size`
a|`Integer`

a|`background-validation-interval`
a|`java.time.Duration`

a|`acquisition-timeout`
a|`java.time.Duration`

a|`leak-detection-interval`
a|`java.time.Duration`

a|`idle-removal-interval`
a|`java.time.Duration`

a|`transaction-isolation-level`
a|`io.quarkus.agroal.runtime`
`.TransactionIsolationLevel`

a|`enable-metrics`
a|`Boolean`

a|`xa`
a|`Boolean`

a|`validation-query-sql`
a|`Boolean`
|===
// end::update_4_6[]

// tag::update_5_3[]
Hibernate configuration properties.
Prefix `quarkus.hibernate-orm` is skipped.

|===
|Parameter|Description|Values[Default]

a|`dialect`
|Class name of the Hibernate ORM dialect.
a|Not necessary to set.

a|`dialect.storage-engine`
|The storage engine when the dialect supports multiple storage engines.
a|Not necessary to set.

a|`sql-load-script`
|Name of the file containing the SQL statements to execute when starts.
`no-file` force Hibernate to skip SQL import.
a|`import.sql`

a|`batch-fetch-size`
|The size of the batches.
a|-1 disabled.

a|`query.query-plan-cache-max-size`
|The maximum size of the query plan cache.
a|

a|`query.default-null-ordering`
a|Default precedence of null values in `ORDER BY`.
a|[`none`], `first`, `last`.

a|`database.generation`
|Database schema is generation.
a|[`none`], `create`, `drop-and-create`, `drop`, `update`.

a|`database.generation.halt-on-error`
|Stop on the first error when applying the schema.
a|[`flase`], `true`

a|`database.default-catalog`
|Default catalog.
a|

a|`database.default-schema`
|Default Schema.
a|

a|`database.charset`
|Charset.
a|

a|`jdbc.timezone`
|Time Zone JDBC driver.
a|

a|`jdbc.statement-fetch-size`
|Number of rows fetched at a time.
a|

a|`jdbc.statement-batch-size`
|Number of updates sent at a time.
a|

a|`log.sql`
|Show SQL logs
a|[`false`], `true`

a|`log.jdbc-warnings`
|Collect and show JDBC warnings.
a|[`false`], `true`

a|`statistics`
|Enable statiscs collection.
a|[`false`], `true`
|===
// end::update_5_3[]

Database operations:

[source, java]
----
// Insert
Developer developer = new Developer();
developer.name = "Alex";
developer.persist();

// Find All
Developer.findAll().list();

// Find By Query
Developer.find("name", "Alex").firstResult();

// Delete
Developer developer = new Developer();
developer.id = 1;
developer.delete();

// Delete By Query
long numberOfDeleted = Developer.delete("name", "Alex");
----

<<<

Remember to annotate methods with `@Transactional` annotation to make changes persisted in the database.

If queries start with the keyword `from` then they are treated as _HQL_ query, if not then next short form is supported:

* `order by` which expands to `from EntityName order by ...`
* `<columnName>` which expands to `from EntityName where <columnName>=?`
* `<query>` which is expanded to `from EntityName where <query>`

*Static Methods*

`findById`:`Object`::
Returns object or null if not found. Overloaded version with `LockModeType` is provided.

// tag::update_12_1[]
`findByIdOptional`:`Optional<Object>`::
Returns object or `java.util.Optional`.
// end::update_12_1[]

`find`:`String`, [`Object...`, `Map<String, Object>`, `Parameters`]::
Lists of entities meeting given query with parameters set.

`find`:`String`, `Sort`, [`Object...`, `Map<String, Object>`, `Parameters`]::
Lists of entities meeting given query with parameters set sorted by `Sort` attribute/s.

`findAll`:: 
Finds all entities.

`findAll`:`Sort`::
Finds all entities sorted by `Sort` attribute/s.

`stream`:`String`, [`Object...`, `Map<String, Object>`, `Parameters`]::
`java.util.stream.Stream` of entities meeting given query with parameters set.

`stream`:`String`, `Sort`, [`Object...`, `Map<String, Object>`, `Parameters`]::
`java.util.stream.Stream` of entities meeting given query with parameters set sorted by `Sort` attribute/s.

`streamAll`:: 
`java.util.stream.Stream` of all entities.

`streamAll`:`Sort`::
`java.util.stream.Stream` of all entities sorted by `Sort` attribute/s.

`count`:: 
Number of entities.

`count`:`String`, [`Object...`, `Map<String, Object>`, `Parameters`]::
Number of entities meeting given query with parameters set.

`deleteAll`:: 
Number of deleted entities.

`delete`:`String`, [`Object...`, `Map<String, Object>`, `Parameters`]::
Number of deleted entities meeting given query with parameters set.

`persist`:[`Iterable`, `Steram`, `Object...`]::

// tag::update_12_6[]
TIP: `find` methods defines a `withLock(LockModeType)` to define the lock type and `withHint(QueryHints.HINT_CACHEABLE, "true")` to define hints.
// end::update_12_6[]

*Pagination*

// tag::update_9_3[]
[source, java]
----
PanacheQuery<Person> livingPersons = Person
            .find("status", Status.Alive);
livingPersons.page(Page.ofSize(25));

// get the first page
List<Person> firstPage = livingPersons.list();
// get the second page
List<Person> secondPage = livingPersons.nextPage().list();
----
// end::update_9_3[]

// tag::update_2_10[]
If entities are defined in external JAR, you need to enable in these projects the `Jandex` plugin in project.

[source, xml]
----
<plugin>
    <groupId>org.jboss.jandex</groupId>
    <artifactId>jandex-maven-plugin</artifactId>
    <version>1.0.3</version>
    <executions>
        <execution>
            <id>make-index</id>
            <goals>
                <goal>jandex</goal>
            </goals>
        </execution>
    </executions>
    <dependencies>
        <dependency>
            <groupId>org.jboss</groupId>
            <artifactId>jandex</artifactId>
            <version>2.1.1.Final</version>
        </dependency>
    </dependencies>
</plugin>
----
// end::update_2_10[]

*DAO pattern*

// tag::update_2_12[]
Also supports _DAO_ pattern with `PanacheRepository<TYPE>`.

[source, java]
----
@ApplicationScoped
public class DeveloperRepository 
    implements PanacheRepository<Person> {
   public Person findByName(String name){
     return find("name", name).firstResult();
   }
}
----
// end::update_2_12[]

*EntityManager*
// tag::update_5_2[]
You can inject `EntityManager` in your classes:

[source, java]
----
@Inject
EntityManager em;

em.persist(car);
----
// end::update_5_2[]

*Multiple datasources*

// tag::update_9_9[]
You can register more than one datasource.

[source, properties]
----
# default
quarkus.datasource.driver=org.h2.Driver
quarkus.datasource.url=jdbc:h2:tcp://localhost/mem:default
....
# users datasource
quarkus.datasource.users.driver=org.h2.Driver
quarkus.datasource.users.url=jdbc:h2:tcp://localhost/mem:users
----

Notice that after `datasource` you set the datasource name, in previous case `users`.

You can inject then `AgroalDataSource` with `io.quarkus.agroal.DataSource`.

[source, java]
----
@DataSource("users")
AgroalDataSource dataSource1;
----
// end::update_9_9[]

*Flushing*

// tag::update_4_4[]
You can force flush operation by calling `.flush()` or `.persistAndFlush()` to make it in a single call.

IMPORTANT: This flush is less efficient and you still need to commit transaction.
// end::update_4_4[]

*Testing*

// tag::update_4_3[]

There is a Quarkus Test Resource that starts and stops H2 server before and after test suite.

Register dependency `io.quarkus:quarkus-test-h2:test`.

And annotate the test:

[source, java]
----
@QuarkusTestResource(H2DatabaseTestResource.class)
public class FlywayTestResources {
}
----
// end::update_4_3[]

*Transactions*

// tag::update_6_4[]
The easiest way to define your transaction boundaries is to use the `@Transactional` annotation.

Transactions are mandatory in case of none idempotent operations.

[source, java]
----
@Transactional
public void createDeveloper() {}
----

You can control the transaction scope:

* `@Transactional(REQUIRED)` (default): starts a transaction if none was started, stays with the existing one otherwise.

* `@Transactional(REQUIRES_NEW)`: starts a transaction if none was started; if an existing one was started, suspends it and starts a new one for the boundary of that method.

* `@Transactional(MANDATORY)`: fails if no transaction was started ; works within the existing transaction otherwise.

* `@Transactional(SUPPORTS)`: if a transaction was started, joins it ; otherwise works with no transaction.

* `@Transactional(NOT_SUPPORTED)`: if a transaction was started, suspends it and works with no transaction for the boundary of the method; otherwise works with no transaction.

* `@Transactional(NEVER)`: if a transaction was started, raises an exception; otherwise works with no transaction.

You can configure the default transaction timeout using `quarkus.transaction-manager.default-transaction-timeout` configuration property. By default it is set to 60 seconds.

You can set a timeout property, in seconds, that applies to transactions created within the annotated method by using `@TransactionConfiguration` annotation.

[source, java]
----
@Transactional
@TransactionConfiguration(timeout=40)
public void createDeveloper() {}
----

If you want more control over transactions you can inject `UserTransaction` and use a programmatic way.

[source, java]
----
@Inject UserTransaction transaction

transaction.begin();
transaction.commit();
transaction.rollback();
----
// end::update_6_4[]

== Infinispan
// tag::update_8_7[]
Quarkus integrates with https://infinispan.org/[Infinispan, window="_blank"]:

[source, bash]
----
./mvnw quarkus:add-extension 
  -Dextensions="infinispan-client"
----

Serialization uses a library called https://github.com/infinispan/protostream[Protostream, window="_blank"].

*Annotation based*

[source, java]
----
@ProtoFactory
public Author(String name, String surname) {
    this.name = name;
    this.surname = surname;
}

@ProtoField(number = 1)
public String getName() {
    return name;
}

@ProtoField(number = 2)
public String getSurname() {
    return surname;
}
----

Initializer to set configuration settings.

[source, java]
----
@AutoProtoSchemaBuilder(includeClasses = 
    { Book.class, Author.class }, 
    schemaPackageName = "book_sample")
interface BookContextInitializer 
        extends SerializationContextInitializer {
}
----

*User written based*

There are three ways to create your schema:

_Protofile_

Creates a `.proto` file in the `META-INF` directory.

[source, proto]
----
package book_sample;

message Author {
  required string name = 1;
  required string surname = 2;
}
----

In case of having a Collection field you need to use the `repeated` key (ie `repeated Author authors = 4`).

_In code_

Setting `proto` schema directly in a produced bean.

[source, java]
----
@Produces
FileDescriptorSource bookProtoDefinition() {
    return FileDescriptorSource
        .fromString("library.proto",   
                    "package book_sample;\n" +
                    "message Author {\n" +
                    "  required string name = 1;\n" +
                    "  required string surname = 2;\n" +
                    "}");
}
----

_Marshaller_

Using `org.infinispan.protostream.MessageMarshaller` interface.

[source, java]
----
public class AuthorMarshaller 
    implements MessageMarshaller<Author> {

   @Override
   public String getTypeName() {
      return "book_sample.Author";
   }

   @Override
   public Class<? extends Author> getJavaClass() {
      return Author.class;
   }

   @Override
   public void writeTo(ProtoStreamWriter writer, 
                    Author author) throws IOException {
      writer.writeString("name", author.getName());
      writer.writeString("surname", author.getSurname());
   }

   @Override
   public Author readFrom(ProtoStreamReader reader) 
        throws IOException {
      String name = reader.readString("name");
      String surname = reader.readString("surname");
      return new Author(name, surname);
   }
}
----

And producing the marshaller:

[source, java]
----
@Produces
MessageMarshaller authorMarshaller() {
    return new AuthorMarshaller();
}
----
// end::update_8_7[]

*Infinispan Embedded*

// tag::update_10_7[]
[source, bash]
----
./mvnw quarkus:add-extension 
  -Dextensions="infinispan-embeddedy"
----

<<<

Configuration in `infinispan.xml`:

[source, xml]
----
<local-cache name="quarkus-transaction">
   <transaction 
        transaction-manager-lookup=
        "org.infinispan.transaction.lookup.JBossStandaloneJTAManagerLookup"/>
</local-cache>
----

Set configuration file location in `application.properties`:

[source, properties]
----
quarkus.infinispan-embedded.xml-config=infinispan.xml
----

And you can inject the main entry point for the cache:

[source, java]
----
@Inject
org.infinispan.manager.EmbeddedCacheManager cacheManager;
----
// end::update_10_7[]

== Flyway
// tag::update_1_7[]
Quarkus integrates with https://flywaydb.org/[Flyway, window="_blank"] to help you on database schema migrations.

[source, bash]
----
./mvnw quarkus:add-extension 
  -Dextensions="quarkus-flyway"
----

Then place migration files to the migrations folder (`classpath:db/migration`).

You can inject `org.flywaydb.core.Flyway` to programmatically execute the migration.

[source, java]
----
@Inject
Flyway flyway;

flyway.migrate();
----

Or can be automatically executed by setting `migrate-at-start` property to `true`.

[source, properties]
----
quarkus.flyway.migrate-at-start=true
----

List of Flyway parameters.

`quarkus.` as prefix is skipped in the next table.

|===	
|Parameter | Default | Description

`flyway.clean-at-start`::
Execute Flyway clean command (default: `false`) 

`flyway.migrate-at-start`::
Flyway migration automatically (default: `false`)

`flyway.locations`::
CSV locations to scan recursively for migrations. Supported prefixes `classpath` and `filesystem` (default: `classpath:db/migration`).

`flyway.connect-retries`::
The maximum number of retries when attempting to connect (default: 0)

`flyway.schemas`::
CSV case-sensitive list of schemas managed (default: none)

a|`flyway.table`::
The name of Flyway’s schema history table (default: `flyway_schema_history`)

`flyway.sql-migration-prefix`::
Prefix for versioned SQL migrations (default: `V`)

`flyway.repeatable-sql-migration-prefix::`
Prefix for repeatable SQL migrations (default: `R`)

`flyway.baseline-on-migrate`::
Only migrations above *baseline-version* will then be applied

`flyway.baseline-version`::
Version to tag an existing schema with when executing baseline (default: 1)

`flyway.baseline-description`::
Description to tag an existing schema with when executing baseline (default: `Flyway Baseline`)
|===
// end::update_1_7[]

*Multiple Datasources*
// tag::update_12_7[]

To use multiple datasource in Flyway you just need to add the datasource name just after the `flyway` property:

[source, properties]
----
quarkus.datasource.users.url=jdbc:h2:tcp://localhost/mem:users
quarkus.datasource.inventory.url=jdbc:h2:tcp://localhost/mem:inventory
# ...

quarkus.flyway.users.schemas=USERS_TEST_SCHEMA
quarkus.flyway.inventory.schemas=INVENTORY_TEST_SCHEMA
# ...
----
// end::update_12_7[]

== Hibernate Search
// tag::update_3_1[]

Quarkus integrates with https://www.elastic.co/products/elasticsearch[Elasticsearch, window="_blank"] to provide a full-featured full-text search using https://hibernate.org/search/[Hibernate Search, window="_blank"] API. 

[source, bash]
----
./mvnw quarkus:add-extension 
  -Dextensions="quarkus-hibernate-search-elasticsearch"
----

You need to annotate your model with Hibernate Search API to index it:

[source, java]
----
@Entity
@Indexed
public class Author extends PanacheEntity {

    @FullTextField(analyzer = "english")
    public String bio;

    @FullTextField(analyzer = "name")
    @KeywordField(name = "firstName_sort", 
        sortable = Sortable.YES, 
        normalizer = "sort")
    public String firstName;

    @OneToMany
    @IndexedEmbedded
    public List<Book> books;

}
----

IMPORTANT: It is not mandatory to use Panache.

You need to define the analyzers and normalizers defined in annotations.
You only need to implement `ElasticsearchAnalysisConfigurer` interface and configure it.

[source, java]
----
public class MyQuarkusAnalysisConfigurer 
            implements ElasticsearchAnalysisConfigurer {

    @Override
    public void configure(
        ElasticsearchAnalysisDefinitionContainerContext ctx) 
    {
            ctx.analyzer("english").custom()
                .withTokenizer("standard")
                .withTokenFilters("asciifolding", 
                    "lowercase", "porter_stem");

        ctx.normalizer("sort").custom() 
            .withTokenFilters("asciifolding", "lowercase");
    }
}
----

Use Hibernate Search in REST service:

[source, java]
----
public class LibraryResource {

    @Inject
    EntityManager em;

    @Transactional
    public List<Author> searchAuthors(
        @QueryParam("pattern") String pattern) { 
        return Search.getSearchSession(em)
            .search(Author.class)
            .predicate(f ->
                pattern == null || pattern.isEmpty() ?
                    f.matchAll() :
                    f.simpleQueryString()
                        .onFields("firstName", 
                            "lastName", "books.title")
                        .matching(pattern)
                )
            .sort(f -> f.byField("lastName_sort")
            .then().byField("firstName_sort"))
            .fetchHits();
    }
----

IMPORTANT: When not using Hibernate ORM, index data using `Search.getSearchSession(em).createIndexer()` `.startAndWait()` at startup time.

Configure the extension in `application.properties`:

[source, properties]
----
quarkus.hibernate-search.elasticsearch.version=7
quarkus.hibernate-search.elasticsearch.
    analysis-configurer=MyQuarkusAnalysisConfigurer
quarkus.hibernate-search.elasticsearch.
    automatic-indexing.synchronization-strategy=searchable
quarkus.hibernate-search.elasticsearch.
    index-defaults.lifecycle.strategy=drop-and-create
quarkus.hibernate-search.elasticsearch.
    index-defaults.lifecycle.required-status=yellow
----

List of Hibernate-Elasticsearch properties prefixed with `quarkus.hibernate-search.elasticsearch`:

|===	
|Parameter | Description

a|`backends`
|Map of configuration of additional backends.

a|`version`
|Version of Elasticsearch

a|`analysis-configurer`
|Class or name of the neab used to configure.

a|`hosts`
|List of Elasticsearch servers hosts.

a|`username`
|Username for auth.

a|`password`
|Password for auth.

a|`connection-timeout`
|Duration of connection timeout.

a|`max-connections`
|Max number of connections to servers.

a|`max-connections-per-route`
|Max number of connections to server.

a|`indexes`
|Per-index specific configuration.

a|`discovery.enabled`
|Enables automatic discovery.

a|`discovery.refresh-interval`
|Refresh interval of node list.

a|`discovery.default-scheme`
|Scheme to be used for the new nodes.

a|`automatic-indexing.synchronization-strategy`
a|Status for which you wait before considering the operation completed (`queued`,`committed` or `searchable`).

a|`automatic-indexing.enable-dirty-check`
|When enabled, re-indexing of is skipped if the changes are on properties that are not used when indexing. 

a|`index-defaults.lifecycle.strategy`
a|Index lifecycle (`none`, `validate`, `update`, `create`, `drop-and-create`, `drop-abd-create-drop`)

a|`index-defaults.lifecycle.required-status`
a|Minimal cluster status (`green`, `yellow`, `red`)

a|`index-defaults.lifecycle.required-status-wait-timeout`
|Waiting time before failing the bootstrap.

a|`index-defaults.refresh-after-write`
|Set if index should be refreshed after writes.
|===

Possible annotations:

|===	
|Parameter | Description

a|`@Indexed`
|Register entity as full text index

a|`@FullTextField`
|Full text search. Need to set an analyzer to split tokens.

a|`@KeywordField`
|The string is kept as one single token but can be normalized.

a|`IndexedEmbedded`
|Include the Book fields into the Author index.

a|`@ContainerExtraction`
a|Sets how to extract a value from container, e.g from a `Map`.

a|`@DocumentId`
| Map an unusual entity identifier to a document identifier.

a|`@GenericField`
|Full text index for any supported type.

a|`@IdentifierBridgeRef`
a|Reference to the identifier bridge to use for a `@DocumentId`.

a|`@IndexingDependency`
|How a dependency of the indexing process to a property should affect automatic reindexing.

a|`@ObjectPath`
|

a|`@ScaledNumberField`
a| For `java.math.BigDecimal` or `java.math.BigInteger` that you need higher precision.
|===
// end::update_3_1[]

== Amazon DynamoDB
// tag::update_5_6[]
// tag::update_8_3[]

Quarkus integrates with https://aws.amazon.com/dynamodb/[Amazon DynamoDB, window="_blank"]:

[source, bash]
----
./mvnw quarkus:add-extension 
  -Dextensions="quarkus-amazon-dynamodb"
----

[source, java]
----
@Inject
DynamoDbClient dynamoDB;

@Inject
DynamoDbAsyncClient dynamoDB;
----

To use it as a local DynamoDB instance:

[source, properties]
----
quarkus.dynamodb.region=
    eu-central-1
quarkus.dynamodb.endpoint-override=
    http://localhost:8000
quarkus.dynamodb.credentials.type=STATIC
quarkus.dynamodb.credentials.static-provider
    .access-key-id=test-key
quarkus.dynamodb.credentials.static-provider
.secret-access-key=test-secret
----

If you want to work with an AWS account, you’d need to set it with:

[source, java]
----
quarkus.dynamodb.region=<YOUR_REGION>
quarkus.dynamodb.credentials.type=DEFAULT
----

`DEFAULT` credentials provider chain: 

* System properties `aws.accessKeyId`, `aws.secretKey`
* Env. Varables `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`
* Credentials profile `~/.aws/credentials`
* Credentials through the Amazon EC2 container service if the `AWS_CONTAINER_CREDENTIALS_RELATIVE_URI` set
* Credentials through Amazon EC2 metadata service.

Configuration parameters prefixed with `quarkus.dynamodb`:

|===	
|Parameter | Default | Description
a|`enable-endpoint-discovery`
a|`false`
a|Endpoint discovery for a service API that supports endpoint discovery.

a|`endpoint-override`
a|
a|Configure the endpoint with which the SDK should communicate.

a|`api-call-timeout`
a|
a|Time to complete an execution.

a|`interceptors`
a|
a|List of class interceptors.
|===

Configuration parameters prefixed with `quarkus.dynamodb.aws`:

|===	
|Parameter | Default | Description

a|`region`
a|
a|Region that hosts DynamoDB.

a|`credentials.type`
a|`DEFAULT`
a| Credentials that should be used `DEFAULT`, `STATIC`, `SYSTEM_PROPERTY`, `ENV_VARIABLE`, `PROFILE`, `CONTAINER`, `INSTANCE_PROFILE`, `PROCESS`, `ANONYMOUS`
|===

Credentials specific parameters prefixed with `quarkus.dynamodb.aws.credentials`:

|===	
|Parameter | Default | Description

3+|DEFAULT  

a|`default-provider.async-credential-update-enabled`
a|`false`
a|Should fetch credentials async.

a|`default-provider.reuse-last-provider-enabled`
a|`true`
a|Should reuse the last successful credentials.

3+|STATIC

a|`static-provider.access-key-id`
a|
a|AWS access key id.

a|`static-provider.secret-access-key`
a|
a|AWS secret access key.

3+|PROFILE

a|`profile-provider.profile-name`
a|`default`
a|The name of the profile to use.

3+|PROCESS

a|`process-provider.command`
a|
a|Command to execute to retrieve credentials.

a|`process-provider.process-output-limit`
a|1024
a|Max bytes to retrieve from process.

a|`process-provider.credential-refresh-threshold`
a|`PT15S`
a|The amount of time between credentials expire and credentials refreshed.

a|`process-provider.async-credential-update-enabled`
a|`false`
a|Should fetch credentials async.
|===

In case of synchronous client, the next parameters can be configured prefixed by `quarkus.dynamodb.sync-client`:

|===	
|Parameter | Default | Description

a|`connection-acquisition-timeout`
a|`10S`
a|Connection acquisation timeout.

a|`connection-max-idle-time`
a|`60S`
a|Max time to connection to be opened.

a|`connection-timeout`
a|
a|Connection timeout.

a|`connection-time-to-live`
a|`0`
a|Max time connection to be open.

a|`socket-timeout`
a|`30S`
a|Time to wait for data.

a|`max-connections`
a|`50`
a|Max connections.

a|`expect-continue-enabled`
a|`true`
a|Client send an HTTP `expect-continue` handsake.

a|`use-idle-connection-reaper`
a|`true`
a| Connections in pool should be closed asynchronously.

a|`proxy.endpoint`
a|
a|Endpoint of the proxy server.

a|`proxy.enabled`
a|`false`
a|Enables HTTP proxy.

a|`proxy.username`
a|
a|Proxy username.

a|`proxy.password`
a|
a|Proxy password.

a|`proxy.ntlm-domain`
a|
a|For NTLM, domain name.

a|`proxy.ntlm-workstation`
a|
a|For NTLM, workstation name.

a|`proxy.preemptive-basic-authentication-enabled`
a|
a|Authenticate pre-emptively.

a|`proxy.non-proxy-hosts`
a|
a|List of non proxy hosts.

a|`tls-managers-provider.type`
a|`system-property`
a|TLS manager: `none`, `system-property`, `file-store`

a|`tls-managers-provider.file-store.path`
a|
a|Path to key store.

a|`tls-managers-provider.file-store.type`
a|
a|Key store type.

a|`tls-managers-provider.file-store.password`
a|
a|Key store password.
|===

In case of asynchronous client, the next parameters can be configured prefixed by `quarkus.dynamodb.async-client`:

|===	
|Parameter | Default | Description

a|`connection-acquisition-timeout`
a|`10S`
a|Connection acquisation timeout.

a|`connection-max-idle-time`
a|`60S`
a|Max time to connection to be opened.

a|`connection-timeout`
a|
a|Connection timeout.

a|`connection-time-to-live`
a|`0`
a|Max time connection to be open.

a|`max-concurrency`
a|`50`
a|Max number of concurrent connections.

a|`use-idle-connection-reaper`
a|`true`
a|Connections in pool should be closed asynchronously.

a|`read-timeout`
a|`30S`
a|Read timeout.

a|`write-timeout`
a|`30S`
a|Write timeout.

a|`proxy.endpoint`
a|
a|Endpoint of the proxy server.

a|`proxy.enabled`
a|`false`
a|Enables HTTP proxy.

a|`proxy.non-proxy-hosts`
a|
a|List of non proxy hosts.

a|`tls-managers-provider.type`
a|`system-property`
a|TLS manager: `none`, `system-property`, `file-store`

a|`tls-managers-provider.file-store.path`
a|
a|Path to key store.

a|`tls-managers-provider.file-store.type`
a|
a|Key store type.

a|`tls-managers-provider.file-store.password`
a|
a|Key store password.

a|`ssl-provider`
a|
a|SSL Provider (`jdk`, `openssl`, `openssl-refcnt`).

a|`protocol`
a|`HTTP_1_1`
a|Sets the HTTP protocol.

a|`max-http2-streams`
a|
a|Max number of concurrent streams.

a|`event-loop.override`
a|`false`
a| Enable custom event loop conf.

a|`event-loop.number-of-threads`
a|
a|Number of threads to use in event loop.

a|`event-loop.thread-name-prefix`
a|`aws-java-sdk-NettyEventLoop`
a| Prefix of thread names.
|===
// end::update_5_6[]
// end::update_8_3[]

== Neo4j
// tag::update_5_7[]
Quarkus integrates with https://neo4j.com/[Neo4j, window="_blank"]:

[source, bash]
----
./mvnw quarkus:add-extension 
  -Dextensions="quarkus-neo4j"
----

[source, java]
----
@Inject
org.neo4j.driver.Driver driver;
----

Configuration properties:

`quarkus.neo4j` as prefix is skipped in the next table.

|===	
|Parameter | Default | Description

a|`uri`
a|`localhost:7687`
|URI of Neo4j.

a|`authentication`
`.username`
a|`neo4j`
|Username.

a|`authentication`
`.password`
a|`neo4j`
|Password.

a|`authentication`
`.disabled`
a|`false`
|Disable authentication.

a|`pool.metrics-enabled`
a|`false`
|Enable metrics.

a|`pool.log-leaked-sessions`
a|`false`
|Enable leaked sessions logging.

a|`pool.max-connection-pool-size`
a|`100`
|Max amount of connections.

a|`pool.max-connection-lifetime`
a|`1H`
|Pooled connections older will be closed and removed from the pool.

a|`pool.connection-acquisition-timeout`
a|`1M`
|Timout for connection adquisation.

a|`pool.idle-time-before-connection-test`
a|`-1`
|Pooled connections idled in the pool for longer than this timeout will be tested before they are used.
|===

As Neo4j uses SSL communication by default, to create a native executable you need to compile with next options GraalVM options:

`-H:EnableURLProtocols=http,https --enable-all-security-services -H:+JNI`

And Quarkus Maven Plugin with next configuration:

[source, xml]
----
<artifactId>quarkus-maven-plugin</artifactId>
<executions>
    <execution>
        <id>native-image</id>
        <goals>
            <goal>native-image</goal>
        </goals>
        <configuration>
            <enableHttpUrlHandler>true
            </enableHttpUrlHandler>
            <enableHttpsUrlHandler>true
            </enableHttpsUrlHandler>
            <enableAllSecurityServices>true
            </enableAllSecurityServices>
            <enableJni>true</enableJni>                
        </configuration>
    </execution>
</executions>
----

Alternatively, and as a not recommended way in production, you can disable SSL and Quarkus will disable Bolt SSL as well. `quarkus.ssl.native=false`.

If you are using Neo4j 4.0, you can use fully reactive. 
Add next depenency management `io.projectreactor:reactor-bom:Californium-SR4:pom:import` and dependency: `io.projectreactor:reactor-core`.

[source, java]
----
public Publisher<String> get() {
        return Flux.using(driver::rxSession, ...);
}
----
// end::update_5_7[]

== MongoDB Client
// tag::update_5_10[]
Quarkus integrates with https://www.mongodb.com/[MongoDB, window="_blank"]:

[source, bash]
----
./mvnw quarkus:add-extension 
  -Dextensions="quarkus-mongodb-client"
----

[source, java]
----
@Inject
com.mongodb.client.MongoClient client;

@Inject
io.quarkus.mongodb.ReactiveMongoClient client;
----

[source, properties]
----
quarkus.mongodb.connection-string=mongodb://localhost:27018
quarkus.mongodb.write-concern.journal=false
----

`quarkus.mongodb` as prefix is skipped in the next table.

|===	
|Parameter | Type | Description

a|`connection-string`
a|`String`
a|MongoDB connection URI.

a|`hosts`
a|`List<String>`
a|Addresses passed as `host:port`.

a|`application-name`
a|`String`
|Application name.

a|`max-pool-size`
a|`Int`
|Maximum number of connections.

a|`min-pool-size`
a|`Int`
|Minimum number of connections.

a|`max-connection-idle-time`
a|`Duration`
|Idle time of a pooled connection.

a|`max-connection-life-time`
a|`Duration`
|Life time of pooled connection.

a|`wait-queue-timeout`
a|`Duration`
|Maximum wait time for new connection.

a|`maintenance-frequency`
a|`Duration`
|Time period between runs of maintenance job.

a|`maintenance-initial-delay`
a|`Duration`
|Time to wait before running the first maintenance job.

a|`wait-queue-multiple`
a|`Int`
a|Multiplied with `max-pool-size` gives max numer of threads waiting.

a|`connection-timeout`
a|`Duration`
|

a|`socket-timeout`
a|`Duration`
|

a|`tls-insecure`
a|`boolean [false]`
|Insecure TLS.

a|`tls`
a|`boolean [false]`
|Enable TLS

a|`replica-set-name`
a|`String`
|Implies hosts given are a seed list.

a|`server-selection-timeout`
a|`Duration`
|Time to wait for server selection.

a|`local-threshold`
a|`Duration`
|Minimum ping time to make a server eligible.

a|`heartbeat-frequency`
a|`Duration`
|Frequency to determine the state of servers.

a|`read-preference`
a|
`primary`,
`primaryPreferred`,
`secondary`,
`secondaryPreferred`,
`nearest`
|Read preferences.

a|`max-wait-queue-size`
a|`Int`
|Max number of concurrent operations allowed to wait.

a|`write-concern.safe`
a|`boolean [true]`
|Ensures are writes are ack.

a|`write-concern.journal`
a|`boolean [true]`
|Journal writing aspect.

a|`write-concern.w`
a|`String`
|Value to all write commands.

a|`write-concern.retry-writes`
a|`boolean [false]`
|Retry writes if network fails.

a|`write-concern.w-timeout`
a|`Duration`
|Timeout to all write commands.

a|`credentials.username`
a|`String`
|Username.

a|`credentials.password`
a|`String`
|Password.

a|`credentials.auth-mechanism`
a|`MONGO-CR`, `GSSAPI`, `PLAIN`, `MONGODB-X509`
|

a|`credentials.auth-source`
a|`String`
|Source of the authentication credentials.

a|`credentials.auth-mechanism-properties`
a|`Map<String, String>`
|Authentication mechanism properties.
|===
// end::update_5_10[]

== MongoDB Panache
// tag::update_9_2[]
You can also use the Panache framework to write persistence part when using MongoDB.

[source, bash]
----
./mvnw quarkus:add-extension 
  -Dextensions="mongodb-panache"
----

MongoDB configuration comes from <<MongoDB Client>> section.

[source, java]
----
@MongoEntity(collection="ThePerson")
public class Person extends PanacheMongoEntity {
    public String name;

    @BsonProperty("birth")
    public LocalDate birthDate;

    public Status status;
}
----

Possible annotations in fields: `@BsonId` (for custom ID), `@BsonProperty` and `@BsonIgnore`.

*Important:* `@MongoEntity` is optional.

Methods provided are similar of the ones shown in <<Persistence>> section.


[source, java]
----
person.persist();
person.update();
person.delete();

List<Person> allPersons = Person.listAll();
person = Person.findById(personId);
List<Person> livingPersons = Person.list("status", Status.Alive);
List<Person> persons = Person.list(Sort.by("name").and("birth"));

long countAll = Person.count();

Person.delete("status", Status.Alive);
----

All `list` methods have equivalent `stream` versions.

*Pagination*

You can also use pagination:

[source, java]
----
PanacheQuery<Person> livingPersons = 
    Person.find("status", Status.Alive);
livingPersons.page(Page.ofSize(25));

// get the first page
List<Person> firstPage = livingPersons.list();
// get the second page
List<Person> secondPage = livingPersons.nextPage().list();
----

*Queries*

Native MongoDB queries are supported (if they start with `{` or `org.bson.Document` instance) as well as Panache Queries.
Panache Queries equivalence in MongoDB:

* `firstname = ?1 and status = ?2` -> `{'firstname': ?1, 'status': ?2}`
* `amount > ?1 and firstname != ?2` -> `{'amount': {'$gt': ?1}, 'firstname': {'$ne': ?2}}`
* `lastname like ?1` -> `{'lastname': {'$regex': ?1}}`
* `lastname is not null` -> `{'lastname':{'$exists': true}}`

WARNING: PanacheQL refers to the Object parameter name but native queries refer to MongoDB field names.

*DAO pattern*

[source, java]
----
@ApplicationScoped
public class PersonRepository 
    implements PanacheMongoRepository<Person> {
}
----

If entities are defined in external JAR, you need to enable in these projects the `Jandex` plugin in project.

[source, xml]
----
<plugin>
    <groupId>org.jboss.jandex</groupId>
    <artifactId>jandex-maven-plugin</artifactId>
    <version>1.0.3</version>
    <executions>
        <execution>
            <id>make-index</id>
            <goals>
                <goal>jandex</goal>
            </goals>
        </execution>
    </executions>
    <dependencies>
        <dependency>
            <groupId>org.jboss</groupId>
            <artifactId>jandex</artifactId>
            <version>2.1.1.Final</version>
        </dependency>
    </dependencies>
</plugin>
----
// end::update_9_2[]
